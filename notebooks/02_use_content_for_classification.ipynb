{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model.network import Converter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import lpips_tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers, losses, regularizers\n",
    "from keras.layers import Conv2D, Dense, UpSampling2D, LeakyReLU, Activation\n",
    "from keras.layers import Layer, Input, Reshape, Lambda, Flatten, Concatenate, Embedding, GaussianNoise\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from keras.applications import vgg16\n",
    "from keras_lr_multiplier import LRMultiplier\n",
    "from assets import AssetManager\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks.callbacks import EarlyStopping, CSVLogger\n",
    "from keras.callbacks.tensorboard_v1 import TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORDContentClassifier:\n",
    "    def __init__(self, subset=None,\n",
    "                 base_dir = 'results', model_name = 'minst_10_model', data_name = 'minst_10_test', include_encoders=True):\n",
    "        assets = AssetManager(base_dir)\n",
    "        data = np.load(assets.get_preprocess_file_path(data_name))\n",
    "        imgs, self.classes, self.contents, n_classes = data['imgs'], data['classes'], data['contents'], data['n_classes']\n",
    "        imgs = imgs.astype(np.float32) / 255.0\n",
    "        if subset is not None:\n",
    "            self.curr_imgs = imgs[:subset]\n",
    "            self.classes = self.classes[:subset]\n",
    "        else:\n",
    "            self.curr_imgs = imgs\n",
    "            self.classes = self.classes\n",
    "\n",
    "        self.onehot_enc = OneHotEncoder()\n",
    "        self.onehot_classes = self.onehot_enc.fit_transform(self.classes.reshape(-1,1))\n",
    "        self.n_classes = self.onehot_classes.shape[1]\n",
    "\n",
    "        self.n_images = self.curr_imgs.shape[0]\n",
    "        \n",
    "        self.converter = Converter.load( assets.get_model_dir(model_name), include_encoders=include_encoders)\n",
    "        self.content_codes = self.converter.content_encoder.predict(self.curr_imgs)\n",
    "        class_codes = self.converter.class_encoder.predict(self.curr_imgs)\n",
    "        class_adain_params = self.converter.class_modulation.predict(class_codes)\n",
    "        self.class_adain_params = class_adain_params.reshape(class_adain_params.shape[0], -1)\n",
    "        \n",
    "    def train_content_classifier(self, n_epochs):        \n",
    "        model = self.get_model(self.content_codes.shape[1])        \n",
    "        callbacks = [EarlyStopping('val_accuracy', patience=10), CSVLogger('LORDContentClassifier_content.csv'), TensorBoard()]\n",
    "        return model.fit(self.content_codes, self.onehot_classes, epochs=n_epochs, validation_split=0.3, callbacks=callbacks)\n",
    "        \n",
    "    def train_class_classifier(self, n_epochs):        \n",
    "        model = self.get_model(self.class_adain_params.shape[1])        \n",
    "        callbacks = [EarlyStopping('val_accuracy', patience=10), CSVLogger('LORDContentClassifier_class.csv'), TensorBoard()]\n",
    "        return model.fit(self.class_adain_params, self.onehot_classes, epochs=n_epochs, validation_split=0.3, callbacks=callbacks)\n",
    "        \n",
    "    def get_model(self, input_dim):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=256, activation='relu', input_dim=input_dim))\n",
    "        model.add(Dense(units=self.n_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading models...\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "vgg arch:\n",
      "Model: \"vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "vgg_normalization_1 (VggNorm (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              [(None, 64, 64, 64), (Non 14714688  \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 14 samples, validate on 6 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/10000\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 3.0981 - accuracy: 0.0714 - val_loss: 3.1565 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/10000\n",
      "14/14 [==============================] - 0s 283us/step - loss: 2.8174 - accuracy: 0.1429 - val_loss: 3.1924 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10000\n",
      "14/14 [==============================] - 0s 282us/step - loss: 2.5535 - accuracy: 0.1429 - val_loss: 3.2317 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10000\n",
      "14/14 [==============================] - 0s 286us/step - loss: 2.3074 - accuracy: 0.3571 - val_loss: 3.2743 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10000\n",
      "14/14 [==============================] - 0s 360us/step - loss: 2.0801 - accuracy: 0.5714 - val_loss: 3.3197 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10000\n",
      "14/14 [==============================] - 0s 215us/step - loss: 1.8742 - accuracy: 0.6429 - val_loss: 3.3704 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10000\n",
      "14/14 [==============================] - 0s 357us/step - loss: 1.6885 - accuracy: 0.8571 - val_loss: 3.4255 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10000\n",
      "14/14 [==============================] - 0s 429us/step - loss: 1.5214 - accuracy: 1.0000 - val_loss: 3.4852 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10000\n",
      "14/14 [==============================] - 0s 429us/step - loss: 1.3709 - accuracy: 1.0000 - val_loss: 3.5505 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10000\n",
      "14/14 [==============================] - 0s 286us/step - loss: 1.2352 - accuracy: 1.0000 - val_loss: 3.6195 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/10000\n",
      "14/14 [==============================] - 0s 285us/step - loss: 1.1133 - accuracy: 1.0000 - val_loss: 3.6925 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "cc = LORDContentClassifier(20, model_name='smallnorb_model', data_name = 'smallnorb_test')\n",
    "res = cc.train_content_classifier(10000)\n",
    "# cc.train_class_classifier(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'epoch',\n",
       " 'history',\n",
       " 'model',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_predict_batch_begin',\n",
       " 'on_predict_batch_end',\n",
       " 'on_predict_begin',\n",
       " 'on_predict_end',\n",
       " 'on_test_batch_begin',\n",
       " 'on_test_batch_end',\n",
       " 'on_test_begin',\n",
       " 'on_test_end',\n",
       " 'on_train_batch_begin',\n",
       " 'on_train_batch_end',\n",
       " 'on_train_begin',\n",
       " 'on_train_end',\n",
       " 'params',\n",
       " 'set_model',\n",
       " 'set_params',\n",
       " 'validation_data']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.692481756210327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.epoch[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
