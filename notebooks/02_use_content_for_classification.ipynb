{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING import wandb.keras called before import keras or import tensorflow.keras.  This can lead to a version mismatch, W&B now assumes tensorflow.keras\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from model.network import Converter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import lpips_tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers, losses, regularizers\n",
    "from keras.layers import Conv2D, Dense, UpSampling2D, LeakyReLU, Activation\n",
    "from keras.layers import Layer, Input, Reshape, Lambda, Flatten, Concatenate, Embedding, GaussianNoise\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from keras.applications import vgg16\n",
    "from keras_lr_multiplier import LRMultiplier\n",
    "from assets import AssetManager\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks.callbacks import EarlyStopping, CSVLogger\n",
    "from keras.callbacks.tensorboard_v1 import TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from content_classification import LORDContentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LORDContentClassifier:\n",
    "#     def __init__(self, subset=None,\n",
    "#                  base_dir = 'results', model_name = 'minst_10_model', data_name = 'minst_10_test', include_encoders=True):\n",
    "#         assets = AssetManager(base_dir)\n",
    "#         data = np.load(assets.get_preprocess_file_path(data_name))\n",
    "#         imgs, self.classes, self.contents, n_classes = data['imgs'], data['classes'], data['contents'], data['n_classes']\n",
    "#         imgs = imgs.astype(np.float32) / 255.0\n",
    "#         if subset is not None:\n",
    "#             self.curr_imgs = imgs[:subset]\n",
    "#             self.classes = self.classes[:subset]\n",
    "#         else:\n",
    "#             self.curr_imgs = imgs\n",
    "#             self.classes = self.classes\n",
    "\n",
    "#         self.onehot_enc = OneHotEncoder()\n",
    "#         self.onehot_classes = self.onehot_enc.fit_transform(self.classes.reshape(-1,1))\n",
    "#         self.n_classes = self.onehot_classes.shape[1]\n",
    "\n",
    "#         self.n_images = self.curr_imgs.shape[0]\n",
    "        \n",
    "#         self.converter = Converter.load( assets.get_model_dir(model_name), include_encoders=include_encoders)\n",
    "#         self.content_codes = self.converter.content_encoder.predict(self.curr_imgs)\n",
    "#         class_codes = self.converter.class_encoder.predict(self.curr_imgs)\n",
    "#         class_adain_params = self.converter.class_modulation.predict(class_codes)\n",
    "#         self.class_adain_params = class_adain_params.reshape(class_adain_params.shape[0], -1)\n",
    "        \n",
    "#     def train_content_classifier(self, n_epochs):        \n",
    "#         model = self.get_model(self.content_codes.shape[1])        \n",
    "#         callbacks = [EarlyStopping('val_accuracy', patience=10), CSVLogger('LORDContentClassifier_content.csv'), TensorBoard()]\n",
    "#         return model.fit(self.content_codes, self.onehot_classes, epochs=n_epochs, validation_split=0.3, callbacks=callbacks)\n",
    "        \n",
    "#     def train_class_classifier(self, n_epochs):        \n",
    "#         model = self.get_model(self.class_adain_params.shape[1])        \n",
    "#         callbacks = [EarlyStopping('val_accuracy', patience=10), CSVLogger('LORDContentClassifier_class.csv'), TensorBoard()]\n",
    "#         return model.fit(self.class_adain_params, self.onehot_classes, epochs=n_epochs, validation_split=0.3, callbacks=callbacks)\n",
    "        \n",
    "#     def get_model(self, input_dim):\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(units=256, activation='relu', input_dim=input_dim))\n",
    "#         model.add(Dense(units=self.n_classes, activation='softmax'))\n",
    "\n",
    "#         model.compile(loss='categorical_crossentropy',\n",
    "#                       optimizer='adam',\n",
    "#                       metrics=['accuracy'])\n",
    "#         return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16 16 16 ...  9  9  9]\n",
      "(38880,) (38880, 64, 64, 1)\n",
      "(100,) (100, 64, 64, 1)\n",
      "loading models...\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "vgg arch:\n",
      "Model: \"vgg\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 1)         0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "vgg_normalization_1 (VggNorm (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              [(None, 64, 64, 64), (Non 14714688  \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "(100,) (100, 64, 64, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (100, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-75a2067459db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLORDContentClassifier\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'smallnorb_no_adain'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'smallnorb_strict_class_fxd_train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_content_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# cc.train_class_classifier(10000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\lord\\content_classification.py\u001b[0m in \u001b[0;36mtrain_content_classifier\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         hist = model.fit(self.content_codes, self.onehot_classes, epochs=n_epochs, validation_split=0.3,\n\u001b[1;32m---> 52\u001b[1;33m                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m     53\u001b[0m         wandb.log({'content_classifier_val_acc': hist.history['val_accuracy'][-1],\n\u001b[0;32m     54\u001b[0m                    'content_classifier_n_epoch': hist.epoch[-1]})\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[1;31m# using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m                 training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m--> 642\u001b[1;33m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    282\u001b[0m                 raise ValueError(\n\u001b[0;32m    283\u001b[0m                     \u001b[1;34m'You are passing a target array of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m                     \u001b[1;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m                     \u001b[1;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                     \u001b[1;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are passing a target array of shape (100, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "cc = LORDContentClassifier( 100, model_name='smallnorb_no_adain', data_name = 'smallnorb_strict_class_fxd_train')\n",
    "cc.train_content_classifier(1000)\n",
    "# cc.train_class_classifier(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
